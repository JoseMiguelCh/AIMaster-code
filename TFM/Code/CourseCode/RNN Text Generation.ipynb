{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"v2guBeDYVFTK","executionInfo":{"status":"ok","timestamp":1649289762391,"user_tz":300,"elapsed":7591,"user":{"displayName":"Jose Miguel Chacón","userId":"06978151070294631058"}}},"outputs":[],"source":["import torch\n","import os\n","import torch.nn as nn\n","import numpy as np\n","from torch.nn.utils import clip_grad_norm"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2RRgEOSLVFTM","executionInfo":{"status":"ok","timestamp":1649289764485,"user_tz":300,"elapsed":210,"user":{"displayName":"Jose Miguel Chacón","userId":"06978151070294631058"}}},"outputs":[],"source":["class Dictionary(object):\n","    def __init__(self):\n","        self.word2idx = {}\n","        self.idx2word = {}\n","        self.idx = 0\n","\n","    def add_word(self, word):\n","        if word not in self.word2idx:\n","            self.word2idx[word] = self.idx\n","            self.idx2word[self.idx] = word\n","            self.idx += 1\n","            \n","    def __len__(self):\n","        return len(self.word2idx)"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"huVp2CH8VFTM","executionInfo":{"status":"ok","timestamp":1649289766969,"user_tz":300,"elapsed":150,"user":{"displayName":"Jose Miguel Chacón","userId":"06978151070294631058"}}},"outputs":[],"source":["class TextProcess(object):\n","    \n","    def __init__(self):\n","        self.dictionary = Dictionary()\n","\n","    def get_data(self, path, batch_size=20):\n","        with open(path, 'r') as f:\n","            tokens = 0\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                tokens += len(words)\n","                for word in words: \n","                    self.dictionary.add_word(word)  \n","        #Create a 1-D tensor that contains the index of all the words in the file\n","        rep_tensor = torch.LongTensor(tokens)\n","        index = 0\n","        with open(path, 'r') as f:\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                for word in words:\n","                    rep_tensor[index] = self.dictionary.word2idx[word]\n","                    index += 1\n","        #Find out how many batches we need            \n","        num_batches = rep_tensor.shape[0] // batch_size     \n","        #Remove the remainder (Filter out the ones that don't fit)\n","        rep_tensor = rep_tensor[:num_batches*batch_size]\n","        # return (batch_size,num_batches)\n","        rep_tensor = rep_tensor.view(batch_size, -1)\n","        return rep_tensor"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"G-gBiYk1VFTN","executionInfo":{"status":"ok","timestamp":1649289769377,"user_tz":300,"elapsed":4,"user":{"displayName":"Jose Miguel Chacón","userId":"06978151070294631058"}}},"outputs":[],"source":["embed_size = 128    #Input features to the LSTM\n","hidden_size = 1024  #Number of LSTM units\n","num_layers = 1\n","num_epochs = 20\n","batch_size = 20\n","timesteps = 30\n","learning_rate = 0.002"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"sw8C9rMnVFTN","executionInfo":{"status":"ok","timestamp":1649289771820,"user_tz":300,"elapsed":230,"user":{"displayName":"Jose Miguel Chacón","userId":"06978151070294631058"}}},"outputs":[],"source":["corpus = TextProcess()"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"baWgH2QWVFTN","executionInfo":{"status":"error","timestamp":1649289775363,"user_tz":300,"elapsed":212,"user":{"displayName":"Jose Miguel Chacón","userId":"06978151070294631058"}},"outputId":"6b3cb112-acb0-436e-a326-0fc6d69b162c","colab":{"base_uri":"https://localhost:8080/","height":293}},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-f6adf993d3ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrep_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'alice.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-48762ab5ae30>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path, batch_size)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'alice.txt'"]}],"source":["rep_tensor = corpus.get_data('alice.txt', batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0gk-aF5hVFTO"},"outputs":[],"source":["#rep_tensor is the tensor that contains the index of all the words. Each row contains 1659 words by default \n","print(rep_tensor.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xeESRwuQVFTO"},"outputs":[],"source":["vocab_size = len(corpus.dictionary)\n","print(vocab_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85OyfO7yVFTO"},"outputs":[],"source":["num_batches = rep_tensor.shape[1] // timesteps\n","print(num_batches)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldsHJ2NIVFTP"},"outputs":[],"source":["class TextGenerator(nn.Module):\n","    \n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n","        super(TextGenerator, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","\n","    def forward(self, x, h):\n","        # Perform Word Embedding \n","        x = self.embed(x)\n","        #Reshape the input tensor\n","        #x = x.view(batch_size,timesteps,embed_size)\n","        out, (h, c) = self.lstm(x, h)\n","        # Reshape the output from (samples,timesteps,output_features) to a shape appropriate for the FC layer \n","        # (batch_size*timesteps, hidden_size)\n","        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n","        # Decode hidden states of all time steps\n","        out = self.linear(out)\n","        return out, (h, c)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mL6vL0f2VFTP"},"outputs":[],"source":["model = TextGenerator(vocab_size, embed_size, hidden_size, num_layers)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WUiKCiRVVFTP"},"outputs":[],"source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j7sfRNpFVFTP"},"outputs":[],"source":["def detach(states):\n","    \"\"\"\n","If we have a tensor z,'z.detach()' returns a tensor that shares the same storage\n","as 'z', but with the computation history forgotten. It doesn't know anything\n","about how it was computed. In other words, we have broken the tensor z away from its past history\n","Here, we want to perform truncated Backpropagation\n","TBPTT splits the 1,000-long sequence into 50 sequences (say) each of length 20 and treats each sequence of length 20 as \n","a separate training case. This is a sensible approach that can work well in practice, but it is blind to temporal \n","dependencies that span more than 20 timesteps.\n","    \"\"\"\n","    return [state.detach() for state in states] "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HaJ3B9yVFTP"},"outputs":[],"source":["for epoch in range(num_epochs):\n","    # Set initial hidden and cell states\n","    states = (torch.zeros(num_layers, batch_size, hidden_size),\n","              torch.zeros(num_layers, batch_size, hidden_size))\n","\n","    for i in range(0, rep_tensor.size(1) - timesteps, timesteps):\n","        # Get mini-batch inputs and targets\n","        inputs = rep_tensor[:, i:i+timesteps]  \n","        targets = rep_tensor[:, (i+1):(i+1)+timesteps]\n","        \n","        outputs,_ = model(inputs, states)\n","        loss = loss_fn(outputs, targets.reshape(-1))\n","\n","        model.zero_grad()\n","        loss.backward()\n","        #Perform Gradient Clipping. clip_value (float or int) is the maximum allowed value of the gradients \n","        #The gradients are clipped in the range [-clip_value, clip_value]. This is to prevent the exploding gradient problem\n","        clip_grad_norm(model.parameters(), 0.5)\n","        optimizer.step()\n","              \n","        step = (i+1) // timesteps\n","        if step % 100 == 0:\n","            print ('Epoch [{}/{}], Loss: {:.4f}'\n","                   .format(epoch+1, num_epochs, loss.item()))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"we2g1d9cVFTQ"},"outputs":[],"source":["# Test the model\n","with torch.no_grad():\n","    with open('results.txt', 'w') as f:\n","        # Set intial hidden ane cell states\n","        state = (torch.zeros(num_layers, 1, hidden_size),\n","                 torch.zeros(num_layers, 1, hidden_size))\n","        # Select one word id randomly and convert it to shape (1,1)\n","        input = torch.randint(0,vocab_size, (1,)).long().unsqueeze(1)\n","\n","        for i in range(500):\n","            output, _ = model(input, state)\n","            print(output.shape)\n","            # Sample a word id from the exponential of the output \n","            prob = output.exp()\n","            word_id = torch.multinomial(prob, num_samples=1).item()\n","            print(word_id)\n","            # Replace the input with sampled word id for the next time step\n","            input.fill_(word_id)\n","\n","            # Write the results to file\n","            word = corpus.dictionary.idx2word[word_id]\n","            word = '\\n' if word == '<eos>' else word + ' '\n","            f.write(word)\n","            \n","            if (i+1) % 100 == 0:\n","                print('Sampled [{}/{}] words and save to {}'.format(i+1, 500, 'results.txt'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qelRP2SJVFTQ"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"RNN Text Generation.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}