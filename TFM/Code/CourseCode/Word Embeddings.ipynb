{"cells":[{"cell_type":"markdown","metadata":{"id":"Tabn7bKaU1H2"},"source":["## Word Embeddings"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"AO56hQKKU1H4","executionInfo":{"status":"ok","timestamp":1649289712441,"user_tz":300,"elapsed":7336,"user":{"displayName":"Jose Miguel Chac贸n","userId":"06978151070294631058"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2AdFOul7U1H5","executionInfo":{"status":"ok","timestamp":1649289715945,"user_tz":300,"elapsed":317,"user":{"displayName":"Jose Miguel Chac贸n","userId":"06978151070294631058"}},"outputId":"d0cc792f-9993-40c0-e392-8a86f9e11c50"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([1])\n"]}],"source":["#The indices of the words represent the values in the lookup table\n","word_to_idx = {\"I\": 0, \"love\": 1, \"eating\":2, \"and\":3, \"sleeping\":4}\n","embeddings = nn.Embedding(5, 7)    # 5 words in vocab, embedding size is 7\n","word_index = torch.tensor([word_to_idx[\"love\"]])\n","print(word_index)"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"INp-ZSpFU1H5","executionInfo":{"status":"ok","timestamp":1649289718358,"user_tz":300,"elapsed":333,"user":{"displayName":"Jose Miguel Chac贸n","userId":"06978151070294631058"}},"outputId":"9ea32a3f-2322-429e-8afc-ccb7c9cdf775"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 0.0542, -0.5999, -0.9556, -0.7349, -1.0816, -2.2142,  0.3905]],\n","       grad_fn=<EmbeddingBackward0>)\n","torch.Size([1, 7])\n"]}],"source":["love = embeddings(word_index)\n","print(love)\n","print(love.shape)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vzmzT28nU1H5","executionInfo":{"status":"ok","timestamp":1649289734853,"user_tz":300,"elapsed":368,"user":{"displayName":"Jose Miguel Chac贸n","userId":"06978151070294631058"}},"outputId":"853e684f-fafc-4558-c2d8-4f12d6781845"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([0, 1, 2, 3, 4])\n","tensor([[-0.4282, -1.2600, -1.1115,  0.1841,  1.0841, -1.6038, -1.3816],\n","        [ 0.0542, -0.5999, -0.9556, -0.7349, -1.0816, -2.2142,  0.3905],\n","        [-0.4083, -0.9367,  0.9167, -1.0451,  1.2116, -0.8265, -0.6874],\n","        [-1.0161,  1.4015, -1.7618, -0.1211,  0.0432, -0.3160,  1.5329],\n","        [-0.0814, -2.4030, -2.4079,  1.0995, -1.1535, -1.0737, -0.6146]],\n","       grad_fn=<EmbeddingBackward0>)\n","torch.Size([5, 7])\n"]}],"source":["all_ind = torch.tensor([w for w in range(5)], dtype = torch.long)\n","all_words = embeddings(all_ind)\n","print(all_ind)\n","print(all_words)\n","print(all_words.shape)"]},{"cell_type":"markdown","metadata":{"id":"ciFbaEAhU1H5"},"source":["## Exercise: N-Gram Skip Model\n","### Given a sequence of words, we want to predict the ith word of the sequence: P(w(i)|w(i-1), w(i-2), .....)\n","#### Source: https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#an-example-n-gram-language-modeling  \n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"Word Embeddings.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":0}